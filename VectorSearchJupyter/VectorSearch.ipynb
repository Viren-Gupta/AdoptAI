{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614acc3e-79be-4dce-8015-dcf5466061df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2f2fcf-3203-41a7-8bd1-e8de681fbe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redis://:@localhost:6379\n"
     ]
    }
   ],
   "source": [
    "REDIS_HOST = os.getenv(\"REDIS_HOST\", \"localhost\") # ex: \"redis-18374.c253.us-central1-1.gce.cloud.redislabs.com\"\n",
    "REDIS_PORT = os.getenv(\"REDIS_PORT\", \"6379\")      # ex: 18374\n",
    "REDIS_PASSWORD = os.getenv(\"REDIS_PASSWORD\", \"\")  # ex: \"1TNxTEdYRDgIDKM2gDfasupCADXXXX\"\n",
    "\n",
    "# If SSL is enabled on the endpoint, use rediss:// as the URL prefix\n",
    "REDIS_URL = f\"redis://:{REDIS_PASSWORD}@{REDIS_HOST}:{REDIS_PORT}\"\n",
    "print(REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f2d4e0-43f0-4585-8cd1-eae9df5c5778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "# Connect with the Redis Python Client\n",
    "client = redis.Redis.from_url(REDIS_URL)\n",
    "\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20176197-6242-4ae9-8b95-9905c1f29d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.dbsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e2694f-ca4c-4c9b-9253-b3e0b314de61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set(\"hello\", \"world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78268e64-a5ab-4a18-80de-d54f40d9c3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'world'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54065ec5-cd34-4555-9941-93415681bf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 movie entries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explosive Pursuit</td>\n",
       "      <td>action</td>\n",
       "      <td>7</td>\n",
       "      <td>A daring cop chases a notorious criminal acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Skyfall</td>\n",
       "      <td>action</td>\n",
       "      <td>8</td>\n",
       "      <td>James Bond returns to track down a dangerous n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Fast &amp; Furious 9</td>\n",
       "      <td>action</td>\n",
       "      <td>6</td>\n",
       "      <td>Dom and his crew face off against a high-tech ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Black Widow</td>\n",
       "      <td>action</td>\n",
       "      <td>7</td>\n",
       "      <td>Natasha Romanoff confronts her dark past and f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>John Wick</td>\n",
       "      <td>action</td>\n",
       "      <td>8</td>\n",
       "      <td>A retired hitman seeks vengeance against those...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              title   genre  rating  \\\n",
       "0   1  Explosive Pursuit  action       7   \n",
       "1   2            Skyfall  action       8   \n",
       "2   3   Fast & Furious 9  action       6   \n",
       "3   4        Black Widow  action       7   \n",
       "4   5          John Wick  action       8   \n",
       "\n",
       "                                         description  \n",
       "0  A daring cop chases a notorious criminal acros...  \n",
       "1  James Bond returns to track down a dangerous n...  \n",
       "2  Dom and his crew face off against a high-tech ...  \n",
       "3  Natasha Romanoff confronts her dark past and f...  \n",
       "4  A retired hitman seeks vengeance against those...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "df = pd.read_json(\"movies.json\")\n",
    "print(\"Loaded\", len(df), \"movie entries\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86da1355-e3ee-4e14-8835-d946e4102d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8968b1feaa4f7dba6674d2d0f68660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from redisvl.utils.vectorize import HFTextVectorizer\n",
    "from redisvl.extensions.cache.embeddings import EmbeddingsCache\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "hf = HFTextVectorizer(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache=EmbeddingsCache(\n",
    "        name=\"embedcache\",\n",
    "        ttl=600,\n",
    "        redis_client=client,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14ae3a53-45d3-4e9e-83f7-a0b34f3b4eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>genre</th>\n",
       "      <th>rating</th>\n",
       "      <th>description</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explosive Pursuit</td>\n",
       "      <td>action</td>\n",
       "      <td>7</td>\n",
       "      <td>A daring cop chases a notorious criminal acros...</td>\n",
       "      <td>b'\\x91f|=\\x92a\\n;\\xbd\\x91\\xb7;.\\xcb~\\xbd\\xbcd\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Skyfall</td>\n",
       "      <td>action</td>\n",
       "      <td>8</td>\n",
       "      <td>James Bond returns to track down a dangerous n...</td>\n",
       "      <td>b'\\xa0D\\x9e\\xbdd\\x9b\\x89\\xbc\\xc8\\x16\\x95\\xbc\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Fast &amp; Furious 9</td>\n",
       "      <td>action</td>\n",
       "      <td>6</td>\n",
       "      <td>Dom and his crew face off against a high-tech ...</td>\n",
       "      <td>b'\\x13\\xa5\\xc7\\xbc\\xfa,\\xa2==\\x19H\\xbc2\\xc6t\\x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Black Widow</td>\n",
       "      <td>action</td>\n",
       "      <td>7</td>\n",
       "      <td>Natasha Romanoff confronts her dark past and f...</td>\n",
       "      <td>b'p\\xeb\\x85\\xbd\\x02\\xcdo\\xbd\\xfe\\xe8\\xc2\\xbbK\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>John Wick</td>\n",
       "      <td>action</td>\n",
       "      <td>8</td>\n",
       "      <td>A retired hitman seeks vengeance against those...</td>\n",
       "      <td>b'\\x98;x\\xbb\\x02/\\xc5=\\x19\\x86:;\\xcd\\xd0\\x94&lt;\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id              title   genre  rating  \\\n",
       "0   1  Explosive Pursuit  action       7   \n",
       "1   2            Skyfall  action       8   \n",
       "2   3   Fast & Furious 9  action       6   \n",
       "3   4        Black Widow  action       7   \n",
       "4   5          John Wick  action       8   \n",
       "\n",
       "                                         description  \\\n",
       "0  A daring cop chases a notorious criminal acros...   \n",
       "1  James Bond returns to track down a dangerous n...   \n",
       "2  Dom and his crew face off against a high-tech ...   \n",
       "3  Natasha Romanoff confronts her dark past and f...   \n",
       "4  A retired hitman seeks vengeance against those...   \n",
       "\n",
       "                                              vector  \n",
       "0  b'\\x91f|=\\x92a\\n;\\xbd\\x91\\xb7;.\\xcb~\\xbd\\xbcd\\...  \n",
       "1  b'\\xa0D\\x9e\\xbdd\\x9b\\x89\\xbc\\xc8\\x16\\x95\\xbc\\x...  \n",
       "2  b'\\x13\\xa5\\xc7\\xbc\\xfa,\\xa2==\\x19H\\xbc2\\xc6t\\x...  \n",
       "3  b'p\\xeb\\x85\\xbd\\x02\\xcdo\\xbd\\xfe\\xe8\\xc2\\xbbK\\...  \n",
       "4  b'\\x98;x\\xbb\\x02/\\xc5=\\x19\\x86:;\\xcd\\xd0\\x94<\\...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"vector\"] = hf.embed_many(df[\"description\"].tolist(), as_buffer=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f31ece0-2c56-4acd-9663-488b96527ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing available documents ... ['resources/.DS_Store', 'resources/nke-10k-2023.pdf', 'resources/amzn-10k-2023.pdf', 'resources/jnj-10k-2023.pdf', 'resources/aapl-10k-2023.pdf', 'resources/testset_15.csv', 'resources/retrieval_basic_rag_test.csv', 'resources/2022-chevy-colorado-ebrochure.pdf', 'resources/nvd-10k-2023.pdf', 'resources/testset.csv', 'resources/msft-10k-2023.pdf', 'resources/propositions.json', 'resources/generation_basic_rag_test.csv']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load list of pdfs from a folder\n",
    "data_path = \"resources/\"\n",
    "docs = [os.path.join(data_path, file) for file in os.listdir(data_path)]\n",
    "\n",
    "print(\"Listing available documents ...\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1585d0c-c8d4-4625-ac7e-1b8db74a6257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing. Created 211 chunks of the original pdf resources/nke-10k-2023.pdf\n"
     ]
    }
   ],
   "source": [
    "# pick out the Nike doc for this exercise\n",
    "doc = [doc for doc in docs if \"nke\" in doc][0]\n",
    "\n",
    "# set up the file loader/extractor and text splitter to create chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500, chunk_overlap=0\n",
    ")\n",
    "loader = PyPDFLoader(doc, headers = None)\n",
    "\n",
    "# extract, load, and make chunks\n",
    "chunks = loader.load_and_split(text_splitter)\n",
    "\n",
    "print(\"Done preprocessing. Created\", len(chunks), \"chunks of the original pdf\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3f0546a-2679-4fa7-aced-ac9b21c32526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf885e887cf453096ba0b44137e5e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "from redisvl.utils.vectorize import HFTextVectorizer, BaseVectorizer\n",
    "from redisvl.extensions.cache.embeddings import EmbeddingsCache\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "hf = HFTextVectorizer(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache=EmbeddingsCache(\n",
    "        name=\"embedcache\",\n",
    "        ttl=600,\n",
    "        redis_url=REDIS_URL,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Embed each chunk content\n",
    "embeddings = hf.embed_many([chunk.page_content for chunk in chunks])\n",
    "\n",
    "# Check to make sure we've created enough embeddings, 1 per document chunk\n",
    "len(embeddings) == len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9600779-0829-4793-911c-0e30192029e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.index import SearchIndex\n",
    "\n",
    "\n",
    "index_name = \"redisvl\"\n",
    "\n",
    "schema = {\n",
    "  \"index\": {\n",
    "    \"name\": index_name,\n",
    "    \"prefix\": \"chunk\"\n",
    "  },\n",
    "  \"fields\": [\n",
    "    {\n",
    "        \"name\": \"chunk_id\",\n",
    "        \"type\": \"tag\",\n",
    "        \"attrs\": {\n",
    "            \"sortable\": True\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"content\",\n",
    "        \"type\": \"text\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"text_embedding\",\n",
    "        \"type\": \"vector\",\n",
    "        \"attrs\": {\n",
    "            \"dims\": 384,\n",
    "            \"distance_metric\": \"cosine\",\n",
    "            \"algorithm\": \"hnsw\",\n",
    "            \"datatype\": \"float32\"\n",
    "        }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08703a6c-dd53-45a2-9d50-3238cc102b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index from schema and the client\n",
    "index = SearchIndex.from_dict(schema, redis_url=REDIS_URL)\n",
    "index.create(overwrite=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a160597-8d90-44b4-adce-5e0a6cf39b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rvl index listall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "901b5eb8-0e05-4a15-93b2-95e9fc48b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcdf92c8-053b-4c8d-889b-1c7081c63f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Index Information:\n",
      "╭───────────────┬───────────────┬───────────────┬───────────────┬───────────────╮\n",
      "│ Index Name    │ Storage Type  │ Prefixes      │ Index Options │ Indexing      │\n",
      "├───────────────┼───────────────┼───────────────┼───────────────┼───────────────┤\n",
      "| redisvl       | HASH          | ['chunk']     | []            | 0             |\n",
      "╰───────────────┴───────────────┴───────────────┴───────────────┴───────────────╯\n",
      "Index Fields:\n",
      "╭─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────┬─────────────────╮\n",
      "│ Name            │ Attribute       │ Type            │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │ Field Option    │ Option Value    │\n",
      "├─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┤\n",
      "│ chunk_id        │ chunk_id        │ TAG             │ SEPARATOR       │ ,               │                 │                 │                 │                 │                 │                 │                 │                 │                 │                 │\n",
      "│ content         │ content         │ TEXT            │ WEIGHT          │ 1               │                 │                 │                 │                 │                 │                 │                 │                 │                 │                 │\n",
      "│ text_embedding  │ text_embedding  │ VECTOR          │ algorithm       │ HNSW            │ data_type       │ FLOAT32         │ dim             │ 384             │ distance_metric │ COSINE          │ M               │ 16              │ ef_construction │ 200             │\n",
      "╰─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────╯\n"
     ]
    }
   ],
   "source": [
    "!rvl index info -i redisvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15d073c-e30c-4c8d-b056-1cba696462bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load expects an iterable of dictionaries\n",
    "from redisvl.redis.utils import array_to_buffer\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        'chunk_id': i,\n",
    "        'content': chunk.page_content,\n",
    "        # For HASH -- must convert embeddings to bytes\n",
    "        'text_embedding': array_to_buffer(embeddings[i], dtype='float32')\n",
    "    } for i, chunk in enumerate(chunks)\n",
    "]\n",
    "\n",
    "# RedisVL handles batching automatically\n",
    "keys = index.load(data, id_field=\"chunk_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0f65f00-a512-41a8-88b0-571f67ea64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*=>[KNN 3 @text_embedding $vector AS vector_distance] RETURN 3 chunk_id content vector_distance SORTBY vector_distance ASC DIALECT 2 LIMIT 0 3'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from redisvl.query import VectorQuery\n",
    "\n",
    "query = \"Nike profit margins and company performance\"\n",
    "\n",
    "query_embedding = hf.embed(query)\n",
    "\n",
    "vector_query = VectorQuery(\n",
    "    vector=query_embedding,\n",
    "    vector_field_name=\"text_embedding\",\n",
    "    num_results=3,\n",
    "    return_fields=[\"chunk_id\", \"content\"],\n",
    "    return_score=True\n",
    ")\n",
    "\n",
    "# show the raw redis query\n",
    "str(vector_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a16cd624-988a-40ec-96ea-7147a00a7172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vector_distance</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk:88</td>\n",
       "      <td>0.337694525719</td>\n",
       "      <td>88</td>\n",
       "      <td>Asia Pacific &amp; Latin America 1,932 1,896 2 % 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk:80</td>\n",
       "      <td>0.342052936554</td>\n",
       "      <td>80</td>\n",
       "      <td>Table of Contents\\nCONSOLIDATED OPERATING RESU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk:87</td>\n",
       "      <td>0.357760906219</td>\n",
       "      <td>87</td>\n",
       "      <td>Table of Contents\\nOPERATING SEGMENTS\\nAs disc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id vector_distance chunk_id  \\\n",
       "0  chunk:88  0.337694525719       88   \n",
       "1  chunk:80  0.342052936554       80   \n",
       "2  chunk:87  0.357760906219       87   \n",
       "\n",
       "                                             content  \n",
       "0  Asia Pacific & Latin America 1,932 1,896 2 % 1...  \n",
       "1  Table of Contents\\nCONSOLIDATED OPERATING RESU...  \n",
       "2  Table of Contents\\nOPERATING SEGMENTS\\nAs disc...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# execute the query with RedisVL\n",
    "result=index.query(vector_query)\n",
    "\n",
    "# view the results\n",
    "pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0292091-33a0-47c6-bb0b-d7cc2231ff7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 0.337694525719\n",
      "80 0.342052936554\n",
      "87 0.357760906219\n"
     ]
    }
   ],
   "source": [
    "# paginate through results\n",
    "for result in index.paginate(vector_query, page_size=1):\n",
    "    print(result[0][\"chunk_id\"], result[0][\"vector_distance\"], flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27b2f2c1-ef8f-4d13-b1b7-19f78cbf0d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>payload</th>\n",
       "      <th>vector_distance</th>\n",
       "      <th>chunk_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk:80</td>\n",
       "      <td>None</td>\n",
       "      <td>0.342052936554</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk:83</td>\n",
       "      <td>None</td>\n",
       "      <td>0.378766179085</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk:87</td>\n",
       "      <td>None</td>\n",
       "      <td>0.357760906219</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk:88</td>\n",
       "      <td>None</td>\n",
       "      <td>0.337694525719</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id payload vector_distance chunk_id\n",
       "0  chunk:80    None  0.342052936554       80\n",
       "1  chunk:83    None  0.378766179085       83\n",
       "2  chunk:87    None  0.357760906219       87\n",
       "3  chunk:88    None  0.337694525719       88"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Sort by chunk_id field after vector search limits to topK\n",
    "vector_query = VectorQuery(\n",
    "    vector=query_embedding,\n",
    "    vector_field_name=\"text_embedding\",\n",
    "    num_results=4,\n",
    "    return_fields=[\"chunk_id\"],\n",
    "    return_score=True\n",
    ")\n",
    "\n",
    "# Decompose vector_query into the core query and the params\n",
    "query = vector_query.query\n",
    "params = vector_query.params\n",
    "\n",
    "# Pass query and params direct to index.search()\n",
    "result = index.search(\n",
    "    query.sort_by(\"chunk_id\", asc=True),\n",
    "    params\n",
    ")\n",
    "\n",
    "pd.DataFrame([doc.__dict__ for doc in result.docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29cff8b9-66d9-475d-85cb-6788eb543814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.index import AsyncSearchIndex\n",
    "\n",
    "async_index = AsyncSearchIndex.from_dict(schema, redis_url=REDIS_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c034cfcb-5dc1-4f9f-a20f-00d517dcb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "\n",
    "CHAT_MODEL = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY :\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7707aa14-c143-4a4b-9b78-6b240daac5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def answer_question(index: AsyncSearchIndex, query: str):\n",
    "    \"\"\"Answer the user's question\"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
    "    to public financial 10k documents in order to answer users questions about company\n",
    "    performance, ethics, characteristics, and core information.\n",
    "    \"\"\"\n",
    "\n",
    "    query_vector = hf.embed(query)\n",
    "    # Fetch context from Redis using vector search\n",
    "    context = await retrieve_context(index, query_vector)\n",
    "    # Generate contextualized prompt and feed to OpenAI\n",
    "    response = await openai.AsyncClient().chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": promptify(query, context)}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        seed=42\n",
    "    )\n",
    "    # Response provided by LLM\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "async def retrieve_context(async_index: AsyncSearchIndex, query_vector) -> str:\n",
    "    \"\"\"Fetch the relevant context from Redis using vector search\"\"\"\n",
    "    results = await async_index.query(\n",
    "        VectorQuery(\n",
    "            vector=query_vector,\n",
    "            vector_field_name=\"text_embedding\",\n",
    "            return_fields=[\"content\"],\n",
    "            num_results=3\n",
    "        )\n",
    "    )\n",
    "    content = \"\\n\".join([result[\"content\"] for result in results])\n",
    "    return content\n",
    "\n",
    "\n",
    "def promptify(query: str, context: str) -> str:\n",
    "    return f'''Use the provided context below derived from public financial\n",
    "    documents to answer the user's question. If you can't answer the user's\n",
    "    question, based on the context; do not guess. If there is no context at all,\n",
    "    respond with \"I don't know\".\n",
    "\n",
    "    User question:\n",
    "\n",
    "    {query}\n",
    "\n",
    "    Helpful context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Answer:\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25503dc4-b118-4683-8320-55a38ce9e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of questions\n",
    "questions = [\n",
    "    \"What is the trend in the company's revenue and profit over the past few years?\",\n",
    "    \"What are the company's primary revenue sources?\",\n",
    "    \"How much debt does the company have, and what are its capital expenditure plans?\",\n",
    "    \"What does the company say about its environmental, social, and governance (ESG) practices?\",\n",
    "    \"What is the company's strategy for growth?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e5f380-7480-4ba4-a983-c9190cbd432e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[\n\u001b[1;32m      4\u001b[0m     answer_question(async_index, question) \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m questions\n\u001b[1;32m      5\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[28], line 13\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(index, query)\u001b[0m\n\u001b[1;32m     11\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retrieve_context(index, query_vector)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Generate contextualized prompt and feed to OpenAI\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mAsyncClient()\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mCHAT_MODEL,\n\u001b[1;32m     15\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     16\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[1;32m     17\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: promptify(query, context)}\n\u001b[1;32m     18\u001b[0m     ],\n\u001b[1;32m     19\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     20\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Response provided by LLM\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2676\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   2631\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2673\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   2674\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   2675\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 2676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2677\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2678\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2679\u001b[0m             {\n\u001b[1;32m   2680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2694\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2695\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2696\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2697\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   2698\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2699\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2700\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2701\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2702\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2703\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2704\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2705\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2706\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2708\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2709\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2710\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2711\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2712\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2713\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2714\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2715\u001b[0m             },\n\u001b[1;32m   2716\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2717\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2718\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2719\u001b[0m         ),\n\u001b[1;32m   2720\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2721\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2722\u001b[0m         ),\n\u001b[1;32m   2723\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2724\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2725\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2726\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py:1884\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, content, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1875\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1876\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1877\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1878\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1879\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1880\u001b[0m     )\n\u001b[1;32m   1881\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1882\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, content\u001b[38;5;241m=\u001b[39mcontent, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1883\u001b[0m )\n\u001b[0;32m-> 1884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py:1669\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1666\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1668\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1669\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "results = await asyncio.gather(*[\n",
    "    answer_question(async_index, question) for question in questions\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8c1d993-f0ac-4b65-b2ea-4b15f0c5ef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from redisvl.extensions.llmcache import SemanticCache\n",
    "\n",
    "llmcache = SemanticCache(\n",
    "    name=\"llmcache\",\n",
    "    vectorizer=hf,\n",
    "    redis_url=REDIS_URL,\n",
    "    ttl=120,\n",
    "    distance_threshold=0.2,\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2790f0be-5b11-4e8d-8e2a-7283083ec486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "# Create an LLM caching decorator\n",
    "def cache(func):\n",
    "    @wraps(func)\n",
    "    async def wrapper(index, query_text, *args, **kwargs):\n",
    "        query_vector = llmcache._vectorizer.embed(query_text)\n",
    "\n",
    "        # Check the cache with the vector\n",
    "        if result := llmcache.check(vector=query_vector):\n",
    "            print(\"Cache hit!\")\n",
    "            return result[0]['response']\n",
    "\n",
    "        response = await func(index, query_text, query_vector=query_vector)\n",
    "        llmcache.store(query_text, response, query_vector)\n",
    "        return response\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@cache\n",
    "async def answer_question(index: AsyncSearchIndex, query: str, **kwargs):\n",
    "    \"\"\"Answer the user's question\"\"\"\n",
    "\n",
    "    SYSTEM_PROMPT = \"\"\"You are a helpful financial analyst assistant that has access\n",
    "    to public financial 10k documents in order to answer users questions about company\n",
    "    performance, ethics, characteristics, and core information.\n",
    "    \"\"\"\n",
    "\n",
    "    context = await retrieve_context(index, kwargs[\"query_vector\"])\n",
    "    response = await openai.AsyncClient().chat.completions.create(\n",
    "        model=CHAT_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": promptify(query, context)}\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        seed=42\n",
    "    )\n",
    "    # Response provided by GPT-3.5\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5fed113-1d13-4284-9829-fe324474d65e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat was Nike\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms revenue last year compared to this year??\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m answer_question(async_index, query)\n",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m, in \u001b[0;36mcache.<locals>.wrapper\u001b[0;34m(index, query_text, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache hit!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(index, query_text, query_vector\u001b[38;5;241m=\u001b[39mquery_vector)\n\u001b[1;32m     15\u001b[0m llmcache\u001b[38;5;241m.\u001b[39mstore(query_text, response, query_vector)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[32], line 30\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(index, query, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m SYSTEM_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful financial analyst assistant that has access\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mto public financial 10k documents in order to answer users questions about company\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mperformance, ethics, characteristics, and core information.\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     29\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retrieve_context(index, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mAsyncClient()\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mCHAT_MODEL,\n\u001b[1;32m     32\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     33\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[1;32m     34\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: promptify(query, context)}\n\u001b[1;32m     35\u001b[0m     ],\n\u001b[1;32m     36\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     37\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Response provided by GPT-3.5\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2676\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   2631\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2673\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   2674\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   2675\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 2676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2677\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2678\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2679\u001b[0m             {\n\u001b[1;32m   2680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2694\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2695\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2696\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2697\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   2698\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2699\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2700\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2701\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2702\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2703\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2704\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2705\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2706\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2708\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2709\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2710\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2711\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2712\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2713\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2714\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2715\u001b[0m             },\n\u001b[1;32m   2716\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2717\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2718\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2719\u001b[0m         ),\n\u001b[1;32m   2720\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2721\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2722\u001b[0m         ),\n\u001b[1;32m   2723\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2724\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2725\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2726\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py:1884\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, content, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1875\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1876\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1877\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1878\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1879\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1880\u001b[0m     )\n\u001b[1;32m   1881\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1882\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, content\u001b[38;5;241m=\u001b[39mcontent, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1883\u001b[0m )\n\u001b[0;32m-> 1884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py:1669\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1666\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1668\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1669\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "query = \"What was Nike's revenue last year compared to this year??\"\n",
    "\n",
    "await answer_question(async_index, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0a8151e-0fb5-4095-8325-21338d8bd0ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat was Nike\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms total revenue in the last year compared to now??\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m answer_question(async_index, query)\n",
      "Cell \u001b[0;32mIn[32], line 14\u001b[0m, in \u001b[0;36mcache.<locals>.wrapper\u001b[0;34m(index, query_text, *args, **kwargs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCache hit!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 14\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m func(index, query_text, query_vector\u001b[38;5;241m=\u001b[39mquery_vector)\n\u001b[1;32m     15\u001b[0m llmcache\u001b[38;5;241m.\u001b[39mstore(query_text, response, query_vector)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "Cell \u001b[0;32mIn[32], line 30\u001b[0m, in \u001b[0;36manswer_question\u001b[0;34m(index, query, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m SYSTEM_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are a helpful financial analyst assistant that has access\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124mto public financial 10k documents in order to answer users questions about company\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mperformance, ethics, characteristics, and core information.\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     29\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m retrieve_context(index, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m openai\u001b[38;5;241m.\u001b[39mAsyncClient()\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mCHAT_MODEL,\n\u001b[1;32m     32\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     33\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: SYSTEM_PROMPT},\n\u001b[1;32m     34\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: promptify(query, context)}\n\u001b[1;32m     35\u001b[0m     ],\n\u001b[1;32m     36\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     37\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Response provided by GPT-3.5\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/resources/chat/completions/completions.py:2676\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   2630\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m   2631\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2673\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m not_given,\n\u001b[1;32m   2674\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   2675\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m-> 2676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m   2677\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2678\u001b[0m         body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[1;32m   2679\u001b[0m             {\n\u001b[1;32m   2680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m   2681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   2682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m   2683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m   2684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m   2685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m   2686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m   2687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m   2688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m   2689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m   2690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m   2691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m   2692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m   2693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m   2694\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: prediction,\n\u001b[1;32m   2695\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m   2696\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_key,\n\u001b[1;32m   2697\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_cache_retention\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt_cache_retention,\n\u001b[1;32m   2698\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning_effort\u001b[39m\u001b[38;5;124m\"\u001b[39m: reasoning_effort,\n\u001b[1;32m   2699\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m   2700\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety_identifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: safety_identifier,\n\u001b[1;32m   2701\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m   2702\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m   2703\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m   2704\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m   2705\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m   2706\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m   2707\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m   2708\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m   2709\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m   2710\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m   2711\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m   2712\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m   2713\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mverbosity\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbosity,\n\u001b[1;32m   2714\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_search_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: web_search_options,\n\u001b[1;32m   2715\u001b[0m             },\n\u001b[1;32m   2716\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsStreaming\n\u001b[1;32m   2717\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[1;32m   2718\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParamsNonStreaming,\n\u001b[1;32m   2719\u001b[0m         ),\n\u001b[1;32m   2720\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m   2721\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m   2722\u001b[0m         ),\n\u001b[1;32m   2723\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m   2724\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   2725\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[1;32m   2726\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py:1884\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, content, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1875\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1876\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing raw bytes as `body` is deprecated and will be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1877\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass raw bytes via the `content` parameter instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1878\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1879\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1880\u001b[0m     )\n\u001b[1;32m   1881\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1882\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, content\u001b[38;5;241m=\u001b[39mcontent, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1883\u001b[0m )\n\u001b[0;32m-> 1884\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/openai/_base_client.py:1669\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1666\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39maread()\n\u001b[1;32m   1668\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1669\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "query = \"What was Nike's total revenue in the last year compared to now??\"\n",
    "\n",
    "await answer_question(async_index, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6cfe19-a8b7-4f16-b0df-2fea44644f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
